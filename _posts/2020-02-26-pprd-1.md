---
layout: post
title:  "PAPER_READER_1: Fast R-CNN - P1: Giới thiệu."
date:   2020-02-26 12:17:13 +0700
categories: PAPER_READER
tags: paper fast-r-cnn
comments: 12
---



​	Bài viết dựa trên nội dung *paper*  <a href='https://arxiv.org/pdf/1504.08083.pdf'>Fast R-CNN (Ross Girshick)</a>.



#### **1. Giới thiệu**

* Gần đây, *Deep ConvNet* [2] có nhiều cải tiến đáng kể trong **image classification** và **object detection**. So với **image classification**, **object detection** có nhiều nhiệm vụ cần giải quyết hơn, đi với đó là quá trình *training* phức tạp và chậm hơn.
* **Detection** có 2 thử thách căn bản:
  * *Thứ nhất*, số lượng *"ứng viên"* ***object location*** lớn cần xử lý.
  * *Thứ hai*, các *"ứng viên"* chỉ cung cấp dữ liệu *localization* thô. Do đó ta phải lọc, tiền xử lý để có *localization* chính xác.
* *Trong paper* này, chúng ta sẽ tinh giản hóa quá trình *training* với mục đích hợp nhất hai nhiệm vụ *classification* và *location* làm một.

##### **1.1. R-CNN [3] và SPPnet [1]**

* **R-CNN** [3]:
  * Tuy đạt được *accuracy* tốt nhưng còn nhiều hạn chế:
    * Quá trình *traning* gồm nhiều giai đoạn.
    * *Training* tốn nhiều thời gian và bộ nhớ.
    * *Detection* chậm (47s/image).
  * **R-CNN** [3] chậm vì mạng *ConvNet* [2] của nó cần *forward* qua từng *object proposal* [4], *'without sharing computation [5]*.
* **SPPnet** [1]:
  * Cải thiện tốc độ so với **R-CNN** [3] vì sử dụng *sharing computation* [5].
    * **SPPnet** [1] tính toán *feature map* [7] bằng *Convolution* [14] cho toàn bộ hình ảnh rồi phân loại từng *object proposal* [4] sử dụng *feature vector extracted* [8]. Đặc trưng được trích xuất từ *proposal* [4] bằng *max-pooling* để thu được đầu ra có kích thước cố định. Nhiều output sẽ được hợp lại thành một vector bằng *spatial pyramid pooling* [9]. **SPPnet** [1] nhanh hơn **R-CNN** [3] khoảng 3 lần *training time* và 10-100 lần *test time*.
  * Tuy nhiên, **SPPnet** [1] vẫn còn nhiều hạn chế. Giống như **R-CNN** [3], quá trình *training* phải trải qua nhiều giai đoạn: *extracting features* [8], *fine-tuning* [10], *training SVMs/Softmax* [11, 12] và *bounding-box regression* [13]. Các *feature* được lưu trực tiếp trên ổ cứng. Khi thực hiện *fine-tuning* [10], quá trình *học* không thể update các *layer* phía trước *spatial pyramid pooloing* [9]. Điều này làm giới hạn *accuracy* của mô hình.



##### **1.2. Đóng góp**

* **Fast R-CNN** ra đời nhằm mục đích xây dựng một thuật toán mới khắc phục sự bất lợi của **R-CNN** [3] và **SPPnet** [1], qua đó làm tăng tốc độ và độ chính xác.
* **Fast R-CNN** có một vài cải tiến:
  * Chất lượng *detection* tốt hơn **R-CNN** [3] và **SPPnet** [1].
  * Chỉ *training* duy nhất một giai đoạn, dùng nhiều *loss* cho từng *task*.
  * Quá trình *training* có thể update weight ở mọi layers.
  * Không cần lưu các *feature* trong bộ nhớ *cache*.



#### Tham khảo:

* [1] <a href='https://arxiv.org/pdf/1406.4729.pdf'>SPPnet</a>
* [2] <a href='https://en.wikipedia.org/wiki/Convolutional_neural_network'>Convolution Neural Network</a>
* [3] <a href='https://arxiv.org/pdf/1311.2524.pdf'>R-CNN</a>
* [4] <a href='https://www.quora.com/What-is-the-definition-of-Object-proposal-in-object-detection'>Object proposal</a>
* [5] <a href='https://computer.howstuffworks.com/shared-computing.htm'>Sharing computation</a>
* [6] <a href='https://arxiv.org/pdf/1406.4729.pdf'>SPPnet</a>
* [7] <a href='https://medium.com/@chriskevin_80184/feature-maps-ee8e11a71f9e'>Feature map</a>
* [8] <a href='https://www.researchgate.net/post/in_simple_words_what_do_you_mean_by_feature_vector_in_image_processing'>Feature vector extraction</a>
* [9] <a href='https://www.youtube.com/watch?v=2IoHC_fhrFU'>Spatical pyramid pooling</a>
* [10] <a href='http://wiki.fast.ai/index.php/Fine_tuning'>Fine tuning</a>
* [11] <a href='https://machinelearningcoban.com/2017/04/09/smv/'>SVM</a>
* [12] <a href='https://machinelearningcoban.com/2017/02/17/softmax/'>Softmax</a>
* [13] <a href='https://arxiv.org/pdf/1904.06805.pdf'>Bounding box regression</a>
* [14] <a href='https://khoablog.github.io/2020/02/20/cv-1'>Convolution</a>
