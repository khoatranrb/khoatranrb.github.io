<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
      
      Khoa Blog
      
    </title>
    <link rel="shortcut icon" type="image/x-icon" href="/assets/res/favicon.ico">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/materialize/0.99.0/css/materialize.min.css">
    <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
    <link rel="stylesheet" href="/assets/css/main.css">
    
    
    <link rel="stylesheet" href="/assets/css/page.css">
    
    
    
    <link rel="stylesheet" href="/assets/css/syntax.css">
    <link rel="alternate" title="Beta" href="http://khoablogs.github.io">
    <link rel="sitemap" type="application/xml" title="Sitemap" href="/sitemap.xml">
    <!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Khoa Blog" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Science Blog!" />
<meta property="og:description" content="Science Blog!" />
<link rel="canonical" href="http://localhost:4000/MD/uml3" />
<meta property="og:url" content="http://localhost:4000/MD/uml3" />
<meta property="og:site_name" content="Khoa Blog" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Khoa Blog" />
<meta name="google-site-verification" content="UA-96359860-1" />
<script type="application/ld+json">
{"@type":"WebPage","url":"http://localhost:4000/MD/uml3","headline":"Khoa Blog","description":"Science Blog!","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body>
    <header>
      <nav class="top-nav teal">
        <div class="nav-wrapper">
          <div class="container">
            <a class="page-title" href="/">Khoa Blog</a>
          </div>
        </div>
      </nav>
      <div class="container">
        <a href="#" data-activates="slide-out" class="button-collapse top-nav full hide-on-large-only">
          <i class="material-icons">menu</i>
        </a>
      </div>
      <ul id="slide-out" class="side-nav fixed">
        <li>
          <div class="userView">
            <div class="background"></div>
            <a href="https://github.com/khoatranrb" target="_blank"><img class="circle z-depth-2" src="/assets/res/user.png"></a>
            <span class="white-text name">Khoa Tran</span>
            <span class="white-text email">khoatranrb@gmail.com</span>
          </div>
        </li>
        <li><a class="waves-effect" href="/"><i class="material-icons">home</i>Home</a></li>
        <li><a class="waves-effect" href="/projects"><i class="material-icons">description</i>Projects</a></li>
        <li><a class="waves-effect" href="/categories"><i class="material-icons">sort</i>Categories</a></li>
        <li><a class="waves-effect" href="/tags"><i class="material-icons">label</i>Tags</a></li>
        <li><a class="waves-effect" href="http://khoablogs.github.io" target="_blank"><i class="material-icons">rss_feed</i>Beta</a></li>
        <li><div class="divider"></div></li>
        <li><a class="waves-effect" href="/about"><i class="material-icons">person</i>About</a></li>
        <li><a class="waves-effect" href="/contact"><i class="material-icons">email</i>Contact</a></li>
      </ul>
    </header>
    <main>

<div class="container">
  <div id="page-info">
  <h3></h3>
</div>
<div class="row">
  <h5 id="23-tối-thiểu-rủi-ro-thực-nghiệm-với-xu-hướng-quy-nạp"><strong>2.3. Tối thiểu rủi ro thực nghiệm với xu hướng quy nạp</strong></h5>

<ul>
  <li>
    <p>Ở bài viết trước, chúng ta đã chứng minh phương pháp <em>ERM</em> có thể dẫn đến <em>overfitting</em>. Bây giờ chúng ta sẽ tìm cách cải thiện nó bằng việc tìm các điều kiện để đảm bảo <em>ERM</em> không bị <em>overfit</em>.</p>
  </li>
  <li>
    <p>Một trong những phương pháp chung khi thực hiện <em>ERM</em> là giới hạn không gian tìm kiếm. Cụ thể, bộ học sẽ chọn trước một tập các hàm dự đoán (trước khi nhìn thấy dữ liệu). Tập các hàm dự đoán này gọi là “không gian giả thuyết” (<em>hypothesis class</em>), ký hiệu là $H$. Mỗi hàm $h\in H$ ánh xạ $X\to Y$. Cho trước $S$ và $H$, bộ học $ERM_H$ sử dụng <em>ERM</em> chọn ra hàm dự đoán $h\in H$ sao cho lỗi trên $S$ nhỏ nhất:
\(ERM_H(S)\in \arg \min_{h\in H}L_S(h)\)</p>
  </li>
  <li>
    <p>Lựa chọn không gian hạn chế lý tưởng khi có hiểu biết trước về vấn đề cần học. Ví dụ, về vấn đề dự đoán trái đu đủ chín hay chưa (đã đề cập ở bài viết trước), chúng ta có thể chọn giả thuyết $H$ là tập các hình chữ nhật (với các cạnh song song với $2$ trục). Ở phần dưới, chúng ta sẽ chứng minh $ERM_H$ cho không gian giả thuyết này không bị <em>overfit</em>.</p>
  </li>
  <li>
    <p>Câu hỏi đặt ra bây giờ là: Không gian giả thuyết như nào thì giúp $ERM_H$ tránh được <em>overfitting</em>? Hãy cùng nhau khám phá ở phần còn lại của bài viết.</p>
  </li>
</ul>

<h6 id="231-không-gian-giả-thuyết"><strong>2.3.1. Không gian giả thuyết</strong></h6>

<ul>
  <li>
    <p>Cách thức đơn giản nhất để giới hạn không gian tìm kiếm là đặt giới hạn trên cho kích cỡ của $H$ (số lượng $h\in H$). Chúng ta cần chứng minh với $H$ hữu hạn, $ERM_H$ sẽ không bị <em>overfit</em>, nếu dữ liệu huấn luyện đủ lớn (size của $S$ phụ thuộc vào size của $H$).</p>
  </li>
  <li>
    <p>Với giả thiết $H$ hữu hạn: cho tập huấn luyện $S$, hàm gán nhãn $f:X\to Y$, ký hiệu $h_S$ là kết quả khi áp dụng $ERM_H$ lên $S$:
\(h_S\in \arg \min_{h\in H}L_S(h)\)</p>
  </li>
  <li>
    <p><strong>Định nghĩa</strong> (Giả thuyết tính khả thi):</p>

    <p>Tồn tại $h^<em>\in H$ sao cho $L_{(D,f)}(h^</em>)=0$. Nghĩa là $h^<em>$ dự đoán chính xác tất cả các mẫu trên $D$, do đó $L_S(h^</em>)=0$ nếu $S$ lấy mẫu từ $D$.</p>

    <p>Ý nói rằng, mọi giả thuyết từ $ERM$ đều có thể cho kết quả $L_S(h_S)=0$. Tuy nhiên, cái ta quan tâm là lỗi trên $D$ chứ không phải trên $S$.</p>
  </li>
  <li>
    <p>$L_S$ có thể đại diện cho $L_D$ nếu $m$ điểm thuộc $S$ được lấy mẫu trên $D$ một cách độc lập. Ký hiệu $S\sim D^m$. Tập $S$ được coi là cửa sổ để bộ học hình dung ra $D$ và $f$. $S$ có thể có hoặc không đại diện cho $D$. Tập $S$ càng lớn thì càng có nhiều khả năng đại diện cho $D$.</p>
  </li>
  <li>
    <p>Giả sử với xác suất $\delta$ tập $S$ không đại diện cho $D$, ta có $1-\delta$ là sự tự tin đối với hàm dự đoán $h_S$. Ta ký hiệu $\epsilon$ tượng trưng cho xác suất dự đoán sai của mô hình. Nếu $L_{(D,f)}&gt;\epsilon$ nghĩa là việc học thất bại, ngược lại, nếu $L_{(D,f)}\le\epsilon$ chứng tỏ việc học thành công.</p>
  </li>
  <li>
    <p><strong>Ta cần tìm xác suất cận trên của việc lấy mẫu $m$ dữ liệu dẫn đến việc học thất bại.</strong> Giả sử $S|_x=(x_1,…,x_m)$ là tập huấn luyện. Ta cần tìm:
\(D^m(\{S|_x:L_{(D,f)}(h_S)&gt;\epsilon\})\)</p>

    <ul>
      <li>
        <p>Gọi $H_B$ là tập giả thiết cho kết quả kém:
\(H_B=\{h\in H:L_{(D,f)}(h)&gt;\epsilon\}\)</p>
      </li>
      <li>
        <p>Gọi
\(M=\{S|_x:\exist h\in H_B,L_S(h)=0\}\)
hay
\(M=\cup_{h\in H_B}\{S|_x:L_S(h)=0\}\)
là tập các bộ huấn luyện lừa được thuật toán học. Nghĩa là $h\in H_B$, $L_{(D,f)}(h_S))&gt;\epsilon$ nhưng lại cho kết quả tốt trên $S|_x$. Ta có:
\(\{S|_x:L_{(D,f)}(h_S)&gt;\epsilon\}\subseteq M\)</p>
      </li>
      <li>
        <p>Do đó ta có:
\(D^m(\{S|_x:L_{(D,f)}(h_S)&gt;\epsilon\})\le D^m(M)=D^m(\{S|_x:\exist h\in H_B,L_S(h)=0\})\tag1\)</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Bổ đề</strong> (Union Bound): Cho hai tập $A$, $B$ và phân phối $D$:
\(D(A\cup B)\le D(A)+D(B) \tag2\)</p>

    <ul>
      <li>Áp dụng $(2)$ vào $(1)$ ta được:
\(D^m(\{S|_x:L_{(D,f)}(h_S)&gt;\epsilon\})\le\sum_{h\in H_B}D^m(\{S|_x:L_S(h)=0\}) \tag3\)</li>
    </ul>
  </li>
  <li>
    <p>Vì $S$ lấy mẫu từ $D$ nên ta có:
\(\begin{align}
D^m(\{S|_x:L_S(h)=0\})&amp;=D^m(\{S|_x:\forall i,h(x_i)=f(x_i)\})\\
&amp;=\prod_{i=1}^mD(\{x_i:h(x_i)=f(x_i)\})
\end{align} \tag4\)</p>

    <ul>
      <li>
        <p>Như giả thuyết ban đầu ta có:
\(D(\{x_i:h(x_i)=f(x_i)\})=1-L_{(D,f)}(h)\le1-\epsilon\)</p>
      </li>
      <li>
        <p>Do đó:
\(D^m(\{S|_x:L_S(h)=0\})\le(1-\epsilon)^m\le e^{-\epsilon m} \tag5\)</p>

        <ul>
          <li>
            <p>Chứng minh:</p>

            <p>Có $\epsilon\in[0,1]$, $m&gt;0$,  cần chứng minh: $(1-\epsilon)^m\le e^{-\epsilon m}$.
\(\begin{align}
&amp;(1-\epsilon)^m\le e^{-\epsilon m}\\
&amp;\to \ln[(1-\epsilon)^m]\le \ln[e^{-\epsilon m}]\\
&amp;\to m\ln(1-\epsilon)\le-\epsilon m\\
&amp;\to \ln(1-\epsilon)\le-\epsilon\\
&amp;\to \epsilon+\ln(1-\epsilon)\le0
\end{align}\)
Với $g(\epsilon)=\epsilon+\ln(1-\epsilon)$, ta có:
\(\frac{\partial g}{\partial\epsilon}=1-\frac{1}{1-\epsilon}\le0 \ \forall\epsilon\in[0,1]\)
Do đó:
\(\max_{\epsilon\in[0,1]}g(\epsilon)=g(0)=0\)
Vậy ta đã chứng minh được bất đẳng thức trên.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Từ $(3)(4)(5)$ ta có:
\(D^m(\{S|_x:L_{(D,f)}(h_S)&gt;\epsilon\})\le|H_B|e^{-\epsilon m}\le|H|e^{-\epsilon m}\)</p>
  </li>
  <li>
    <p><strong>Hệ quả:</strong> Cho $H$ là tập giả thuyết hữu hạn, $\delta\in(0,1)$, $\epsilon&gt;0$ và $m$ thỏa mãn
\(m\ge\frac{\log(\frac{|H|}{\delta})}{\epsilon}\)
thì với độ tự tin ít nhất $1-\delta$ thuật toán $ERM_H$ cho hàm dự đoán $h_S$ tốt, nghĩa là $L_{(D,f)}(h_S)&lt;\epsilon$.</p>
  </li>
</ul>

<h5 id="tham-khảo"><strong>Tham khảo:</strong></h5>

<ul>
  <li>[1] <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Chương 2.3 | Understanding Machine Learning from theory to algorithms.</a></li>
  <li>[2] <a href="https://uetailab.com/index.php/2019/10/18/uml2-3-cuc-tieu-hoa-sai-so-thuc-nghiem-tren-khong-gian-gia-thiet-rut-gon-thien-kien-qui-nap-inductive-bias/">UML2.3 – Cực tiểu hóa sai số thực nghiệm trên không gian giả thiết rút gọn | UET_AI_Lab.</a></li>
</ul>

</div>
</div>

    </main>
    <footer class="page-footer teal">
      <div class="container">
        <div class="row">
          <div class="col s12">
            <img src="/assets/res/logo.png" alt="logo"/>
            <p class="grey-text text-lighten-4">Science Blog!
</p>
          </div>
        </div>
      </div>
      <div class="footer-copyright">
        <div class="container">
          &#xA9; 2021 Khoa Blog. All rights reserved. Powered by <a href="https://github.com/khoablog/khoablog.github.io">Khoa</a>.
        </div>
      </div>
    </footer>
    <script src="//code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/materialize/0.99.0/js/materialize.min.js"></script>
    
    
    
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})
      (window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-96359860-1', 'auto');
      ga('send', 'pageview');
    </script>
    
    <script src="/assets/js/main.js"></script>
  </body>
</html>