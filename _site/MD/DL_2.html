<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
      
      Khoa Blog
      
    </title>
    <link rel="shortcut icon" type="image/x-icon" href="/assets/res/favicon.ico">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/materialize/0.99.0/css/materialize.min.css">
    <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
    <link rel="stylesheet" href="/assets/css/main.css">
    
    
    <link rel="stylesheet" href="/assets/css/page.css">
    
    
    
    <link rel="stylesheet" href="/assets/css/syntax.css">
    <link rel="alternate" title="Beta" href="http://khoablogs.github.io">
    <link rel="sitemap" type="application/xml" title="Sitemap" href="/sitemap.xml">
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Khoa Blog" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Science Blog!" />
<meta property="og:description" content="Science Blog!" />
<link rel="canonical" href="http://localhost:4000/MD/DL_2" />
<meta property="og:url" content="http://localhost:4000/MD/DL_2" />
<meta property="og:site_name" content="Khoa Blog" />
<meta name="google-site-verification" content="UA-96359860-1" />
<script type="application/ld+json">
{"@type":"WebPage","url":"http://localhost:4000/MD/DL_2","headline":"Khoa Blog","description":"Science Blog!","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body>
    <header>
      <nav class="top-nav teal">
        <div class="nav-wrapper">
          <div class="container">
            <a class="page-title" href="/">Khoa Blog</a>
          </div>
        </div>
      </nav>
      <div class="container">
        <a href="#" data-activates="slide-out" class="button-collapse top-nav full hide-on-large-only">
          <i class="material-icons">menu</i>
        </a>
      </div>
      <ul id="slide-out" class="side-nav fixed">
        <li>
          <div class="userView">
            <div class="background"></div>
            <a href="https://github.com/khoatranrb" target="_blank"><img class="circle z-depth-2" src="/assets/res/user.png"></a>
            <span class="white-text name">Khoa Tran</span>
            <span class="white-text email">khoatranrb@gmail.com</span>
          </div>
        </li>
        <li><a class="waves-effect" href="/"><i class="material-icons">home</i>Home</a></li>
        <li><a class="waves-effect" href="/projects"><i class="material-icons">description</i>Projects</a></li>
        <li><a class="waves-effect" href="/categories"><i class="material-icons">sort</i>Categories</a></li>
        <li><a class="waves-effect" href="/tags"><i class="material-icons">label</i>Tags</a></li>
        <li><a class="waves-effect" href="http://khoablogs.github.io" target="_blank"><i class="material-icons">rss_feed</i>Beta</a></li>
        <li><div class="divider"></div></li>
        <li><a class="waves-effect" href="/about"><i class="material-icons">person</i>About</a></li>
        <li><a class="waves-effect" href="/contact"><i class="material-icons">email</i>Contact</a></li>
      </ul>
    </header>
    <main>

<div class="container">
  <div id="page-info">
  <h3></h3>
</div>
<div class="row">
  <ul>
  <li>
    <p>We have introduced <em>the matrix decomposition</em> in <a href="https://khoablog.github.io/2020/05/14/dl-1" style="color:green">DL_1</a> which have many applications. In this article, we will discuss about using <em>the matrix decomposition</em> to obtain <em>the best possible approximation</em> of a given matrix, called a <em>matrix of low rank</em>.</p>
  </li>
  <li>
    <p>A <em>low rank approximation</em> can be considered a <em>compression</em> of the data represented by the given matrix.</p>
  </li>
</ul>

<h5 id="28-the-singular-value-decomposition-svd"><strong>2.8 The Singular Value Decomposition (SVD)</strong></h5>

<ul>
  <li>
    <p>We know that a matrix is not square, the <em>eigendecomposition</em> is not define and we must use a <em>SVD</em> instead.</p>
  </li>
  <li>
    <p>Recall about the <em>eigendecomposition</em>, we analyze a matrix $\mathbf{A}$ rely on its <em>eigenvalues</em> and <em>eigenvectors</em>:
\(\mathbf{A}=\mathbf{V}\text{diag}(\vec\lambda)\mathbf{V}^{-1}\tag1\)</p>
  </li>
  <li>
    <p>In <em>SVD</em>, suppose $\mathbf{A}$ is an $m\times n$ matrix, we have:
\(\mathbf{A}_{m\times n}=\mathbf{U}_{m\times m}\mathbf{D}_{m\times n}\mathbf{V}^\top_{n\times n}\tag2\)</p>
  </li>
  <li>
    <p>In there, $\mathbf{D}$ is a <em>diagonal matrix</em>. The element along the diagonal of $\mathbf{D}$, which are $\lambda_1\ge\lambda_2\ge…\ge\lambda_r\ge0=0=…=0$ are known as the <strong>singular values</strong> of $\mathbf{A}$ and $r$ is rank of $\mathbf{A}$.</p>
  </li>
</ul>

<p>
    <img src="https://raw.githubusercontent.com/khoatranrb/Img4Md/master/B/DL2/svd1.png" />
    <center><i>Source: https://machinelearningcoban.com/2017/06/07/svd/</i></center>
</p>

<h6 id="281-the-origin-of-svd"><strong>2.8.1. The origin of SVD</strong></h6>

<ul>
  <li>
    <p>From $(2)$, we have:
\(\begin{align}
\mathbf{A}\mathbf{A}^\top&amp;=\mathbf{U}\mathbf{D}\mathbf{V}^\top(\mathbf{U}\mathbf{D}\mathbf{V}^\top)^\top\\
&amp;=\mathbf{U}\mathbf{D}\mathbf{V}^\top\mathbf{V}\mathbf{D^\top}\mathbf{U^\top}\\
&amp;=\mathbf{U}\mathbf{D}\mathbf{D^\top}\mathbf{U^\top} \ \ \  \text{(because $V$ is a orthogonal matrix, so $V^\top V=I$)}\\
&amp;=\mathbf{U}\mathbf{D}\mathbf{D^\top}\mathbf{U}^{-1} \ \ \ \text{($U$ is a orthogonal matrix, so $U^\top=U^{-1}$)}\tag3
\end{align}\)</p>

    <ul>
      <li>We see that the element in the diagonal of $\mathbf{D}\mathbf{D}^\top$ are $\lambda_1^2, \lambda_2^2,…$ (as we denote in <a href="https://khoablog.github.io/2020/05/14/dl-1" style="color:green">DL_1</a>). Thus, $\lambda_1^2, \lambda_2^2,…$ are the <em>eigenvalues</em> of $\mathbf{A}\mathbf{A}^\top$ and $(3)$ is the <em>eigendecomposition</em> of $\mathbf{A}\mathbf{A}^\top$.</li>
      <li>Because of $\lambda_i^2\ge0$, $\mathbf{A}\mathbf{A}^\top$ is a <em>positive semidefinite matrix</em>. The values $\lambda_1,…$ are called <em>singular values</em> of $\mathbf{A}$.</li>
      <li>The columns of matrix $\mathbf{U}$ are <em>eigenvectors</em> of $\mathbf{A}\mathbf{A}^\top$, called <strong>left-singular vectors</strong> of $\mathbf{A}$.</li>
    </ul>
  </li>
  <li>
    <p>Similarity, we can obtain:
\(\mathbf{A}^\top\mathbf{A}=\mathbf{V}\mathbf{D}\mathbf{D^\top}\mathbf{V}^{-1}\tag4\)
Therefore, we can show that the columns of matrix $\mathbf{V}$ are the <em>eigenvectors</em> of $\mathbf{A}^\top\mathbf{A}$, called <strong>right-singular vectors</strong> of $ \mathbf{A}$.</p>

    <p>Both $\mathbf{U}$ and $\mathbf{V}$ are <em>orthogonal matrices</em>.</p>
  </li>
</ul>

<h6 id="282-compact-svd"><strong>2.8.2. Compact SVD</strong></h6>

<ul>
  <li>
    <p>We can rewrite $(2)$ to:
\(\mathbf{A}=\lambda_1\mathbf{u}_1\mathbf{v}_1^\top+\lambda_2\mathbf{u}_2\mathbf{v}_2^\top+...+\lambda_r\mathbf{u}_r\mathbf{v}_r^\top\tag5\)
where each $\text{rank}(\mathbf{u}_i\mathbf{v}_i^\top)=1$ ($1\le i\le r$).</p>
  </li>
  <li>
    <p>We see that $\mathbf{A}$ depends only on the first $r$ columns of $\mathbf{U}$, $\mathbf{V}$ and $r$ values which are nonzero on the diagonal of $\mathbf{D}$. So:
\(\mathbf{A}=\mathbf{U}_r\mathbf{D}_r(\mathbf{V}_r)^\top\tag6\)
where $\mathbf{U}_r,\mathbf{V}_r$ included first $r$ columns of $\mathbf{U}$ and $\mathbf{V}_r$, $\mathbf{D}_r$ included first $r$ rows of $\mathbf{D}$.</p>
  </li>
</ul>

<h6 id="283-truncated-svd"><strong>2.8.3. Truncated SVD</strong></h6>

<ul>
  <li>
    <p>We already know that the element in the diagonal of $\mathbf{D}$ are nonzero and decrease $\lambda_1\ge\lambda_2\ge…\ge\lambda_r\ge0=0=…=0$. In reality, some value $\lambda_i$ is a big value, the remainder is small, approximately zero. Thus, we can approximate $\mathbf{A}$ by:
\(\begin{align}
\mathbf{A}\approx\mathbf{A}_k&amp;=\mathbf{U}_k\mathbf{D}_k(\mathbf{V}_k)^\top\\
&amp;=\lambda_1\mathbf{u}_1\mathbf{v}_1^\top+\lambda_2\mathbf{u}_2\mathbf{v}_2^\top+...+\lambda_k\mathbf{u}_k\mathbf{v}_k^\top
\end{align}\tag7\)
where $k&lt; r$.</p>
  </li>
  <li>
    <p><strong>Theorem:</strong>
\(||\mathbf{A}-\mathbf{A}_k||_F^2=\sum_{i=k+1}^r\lambda_i^2\tag8\)
It means error of this approximate way is square root of the sum of squares of singular values that we omitted at the end of $\mathbf{D}$.</p>

    <ul>
      <li>
        <p><em>Prove:</em></p>

        <p>We have $||\mathbf{X}||_F^2=\text{trace}(\mathbf{X}\mathbf{X}^\top)$ and $\text{trace}(\mathbf{X}\mathbf{Y})=\text{trace}(\mathbf{Y}\mathbf{X})$. Now we infer:
\(\begin{align}
||\mathbf{A}-\mathbf{A}_k||_F^2&amp;=||\sum_{i=k+1}^r\lambda_i\mathbf{u}_i\mathbf{v}_i^\top||_F^2\tag9\\
&amp;=\text{trace}\{(\sum_{i=k+1}^r\lambda_i\mathbf{u}_i\mathbf{v}_i^\top)(\sum_{i=k+1}^r\lambda_i\mathbf{u}_i\mathbf{v}_i^\top)^\top\}\tag{10}\\
&amp;=\text{trace}\{\sum_{i=k+1}^r\sum_{j=k+1}^r\lambda_i\lambda_j\mathbf{u}_i\mathbf{v}_i^\top\mathbf{v}_j\mathbf{u}_j^\top\}\tag{11}\\
&amp;=\text{trace}\{\sum_{i=k+1}^r\lambda_i^2\mathbf{u}_i\mathbf{u}_i^\top\}\tag{12}\\
&amp;=\text{trace}\{\sum_{i=k+1}^r\lambda_i^2\mathbf{u}_i^\top\mathbf{u}_i\}\tag{13}\\
&amp;=\text{trace}\{\sum_{i=k+1}^r\lambda_i^2\}\tag{14}\\
&amp;=\sum_{i=k+1}^r\lambda_i^2\tag{15}\\
\end{align}\)</p>

        <p>$(11)=(12),(13)=(14)$ because $\mathbf{U},\mathbf{V}$ are orthogonal matrices.</p>

        <p>$(12)=(13)$ because of the commutative property of $\text{trace}$ function.</p>

        <p>$(14)=(15)$ because $\sum_{i=k+1}^r\lambda_i^2$ is a scalar.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Therefore, we obtain:
\(\frac{||\mathbf{A}-\mathbf{A}_k||_F^2}{||\mathbf{A}||_F^2}=\frac{\sum_{i=k+1}^r\lambda_i^2}{\sum_{i=1}^r\lambda_i^2}\tag{16}\)
We see that the bigger the $k$ is, the smaller the lost of $\mathbf{A}_k$ (compare to the origin matrix $\mathbf{A}$) is. So we use <em>Truncated SVD</em> to approximate the origin matrix base on the amount of information we want to keep, called <em>Low-rank approximation</em>.</p>
  </li>
</ul>

<h5 id="advance"><strong><em>Advance</em></strong></h5>

<p><strong>Prove the existence of SVD:</strong></p>

<p><strong>Theorem:</strong> <em>If $\mathbf{A}$ is a real $m\times n$ matrix then there exist orthogonal matrices</em>
\(\begin{align}
\mathbf{U}&amp;=\begin{bmatrix}\mathbf{u}_1&amp;...&amp;\mathbf{u}_m\end{bmatrix}\in\mathcal{R}^{m\times m}\tag{17}\\
\mathbf{V}&amp;=\begin{bmatrix}\mathbf{v}_1&amp;...&amp;\mathbf{v}_n\end{bmatrix}\in\mathcal{R}^{n\times n}\tag{18}
\end{align}\)
<em>such that</em>
\(\mathbf{U}^\top\mathbf{A}\mathbf{V}=\mathbf{D}=\text{diag}(\lambda_1,...,\lambda_r)\in\mathcal{R}^{m\times n}\tag{19}\)
<em>where $r=\min(m,n)$ and $\lambda_1\ge…\ge\lambda_r\ge0$.  Equivalently:</em>
\(\mathbf{A}=\mathbf{U}\mathbf{D}\mathbf{V}^\top\tag{20}\)
<strong>Proof:</strong> Let $\mathbf{x}\in\mathcal{R}^{n},\mathbf{y}\in\mathcal{R}^{m}$ be unit vector. We consider:
\(z=\mathbf{y}^\top\mathbf{A}\mathbf{x}\tag{21}\)</p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>The scalar $z$ must achieve a maximum value on the set $\mathcal{S}={\mathbf{x},\mathbf{y}</td>
          <td>\mathbf{x}\in\mathcal{R}^{n},\mathbf{y}\in\mathcal{R}^{m},</td>
          <td> </td>
          <td>\mathbf{x}</td>
          <td> </td>
          <td>=</td>
          <td> </td>
          <td>\mathbf{y}</td>
          <td> </td>
          <td>=1}$. Let $\mathbf{u}_1,\mathbf{v}_1$ be two unit vectors in $\mathcal{R}^m$ and $\mathcal{R}^n$ respectively where this $z$ is achieved maximum $\lambda_1$:</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>\(\max_{||\mathbf{x}||=||\mathbf{y}||=1}\mathbf{y}^\top\mathbf{A}\mathbf{x}=\mathbf{u}_1^\top\mathbf{A}\mathbf{v}_1=\lambda_1\tag{22}\)</p>
<ul>
  <li>
    <p>The maximum value of $z$ is dot product between $\mathbf{u}_1$ and $\mathbf{Av}_1$ then $\mathbf{u}_1$ is parallel to the vector $\mathbf{Av}_1$. Also, we have $\mathbf{u}_1^\top\mathbf{Av}_1=\mathbf{v}_1^\top\mathbf{A^\top u}_1$. Similarity, we can show that $\mathbf{v}_1$ is parallel to the vector $\mathbf{A^\top u}_1$.</p>
  </li>
  <li>
    <p>$\mathbf{u}_1,\mathbf{v}_1$ can be extended into orthonormal bases for $\mathcal{R}^m,\mathcal{R^n}$. Collect the orthonormal basis vector from $\mathcal{R}^m,\mathcal{R^n}$ into orthogonal matrices $\mathbf{U}_1,\mathbf{V}_1$ respectively, we have:</p>
  </li>
</ul>

\[\mathbf{U}_1^\top\mathbf{AV}_1=\mathbf{S}_1=\begin{bmatrix}\lambda_1&amp;\mathbf{0}^\top\\\mathbf{0}&amp;\mathbf{A}_1\end{bmatrix}\tag{23}\]

<ul>
  <li>We can show that the first column of $\mathbf{AV}<em>1$ is $\mathbf{Av}_1=\lambda_1\mathbf{u}_1$ *(considered as a exercise for you)*. So the first entry of $\mathbf{U}_1^\top\mathbf{AV}_1$ is $\mathbf{u}_1^\top\lambda_1\mathbf{u}_1=\lambda_1$ and its other entries are $\mathbf{u}_j^\top\mathbf{A}\mathbf{v}_1=0$ because $\mathbf{Av}_1$ is parallel to $\mathbf{u}_1$ and $\mathbf{u}_1\bot \mathbf{u}</em>{i(i\ne 1)}$ (because $\mathbf{U_1}$ is orthonormal basis). Thus, we have just explained the result of first column of $\mathbf{S}_1$. Similarity with the first row of $\mathbf{S}_1$, by $\mathbf{u}_1\mathbf{Av}_2=…=\mathbf{u}_1\mathbf{Av}_n=0$. The matrix $\mathbf{A}_1\in\mathcal{R}^{(m-1)\times(n-1)}$. We have:</li>
</ul>

<p>\(\mathbf{U}_2^\top\mathbf{A_1V}_2=\mathbf{S}_2=\begin{bmatrix}\lambda_2&amp;\mathbf{0}^\top\\\mathbf{0}&amp;\mathbf{A}_2\end{bmatrix}\tag{24}\)
​		So:
\(\begin{bmatrix}1&amp;\mathbf{0}^\top\\\mathbf{0}&amp;\mathbf{U}_2^\top\end{bmatrix}\mathbf{U}_1^\top\mathbf{AV}_1\begin{bmatrix}1&amp;\mathbf{0}^\top\\\mathbf{0}&amp;\mathbf{V}_2\end{bmatrix}=\begin{bmatrix}\lambda_1&amp;0&amp;\mathbf{0}^\top\\0&amp;\lambda_2&amp;\mathbf{0}^\top\\ \mathbf{0}&amp;\mathbf{0}&amp;\mathbf{A}_2 \end{bmatrix}\tag{25}\)
​		We repeat until $\mathbf{A}_k$ has zero rows and zeros columns to obtain:
\(\mathbf{U}^\top\mathbf{AV}=\mathbf{D}\tag{26}\)
​		where $\mathbf{U}$ and $\mathbf{V}$ are orthogonal matrices obtained by multiplying together all the orthogonal 		matrices used in the procedure, and:
\(\mathbf{D}=\text{diag}(\lambda_1,...,\lambda_p)\tag{27}\)
​		Equivalently:
\(\mathbf{A}=\mathbf{U}\mathbf{DV}^\top\tag{28}\)
​		which is the desired result.</p>

<ul>
  <li>
    <p>Next, we show that $\lambda_1\ge…\ge\lambda_p$. To show this, we just need to show that $\lambda_2\le\lambda_1$. We have:
\(\begin{align}
\lambda_2&amp;=\max_{||\hat{\mathbf{x}}||=||\hat{\mathbf{y}}||=1}\hat{\mathbf{y}}^\top\mathbf{A}_1\hat{\mathbf{x}}\tag{29}\\
&amp;=\max_{||\hat{\mathbf{x}}||=||\hat{\mathbf{y}}||=1}\begin{bmatrix}0&amp;\hat{\mathbf{y}}\end{bmatrix}^\top\mathbf{S}_1\begin{bmatrix}0\\\hat{\mathbf{x}}\end{bmatrix}\tag{30}\\
&amp;=\max_{||\hat{\mathbf{x}}||=||\hat{\mathbf{y}}||=1}\begin{bmatrix}0&amp;\hat{\mathbf{y}}\end{bmatrix}^\top\mathbf{U}_1^\top\mathbf{AV}_1\begin{bmatrix}0\\\hat{\mathbf{x}}\end{bmatrix}\tag{31}\\
&amp;=\max_{||\hat{\mathbf{x}}||=||\hat{\mathbf{y}}||=1\\\mathbf{x^\top v}_1=\mathbf{y^\top u}_1=0}\mathbf{y^\top Ax}\tag{32}
\end{align}\)
where
\(\mathbf{x}=\mathbf{V}_1\begin{bmatrix}0\\\hat{\mathbf{x}}\end{bmatrix}\tag{33}\)
and
\(\mathbf{y}=\mathbf{U}_1\begin{bmatrix}0\\\hat{\mathbf{y}}\end{bmatrix}\tag{34}\)
We see that $\mathbf{x}$ is a linear combination of $\mathbf{v}_2,…,\mathbf{v}_n$ and therefore it is unit vector and orthogonal to $\mathbf{v}_1$. Similarity, we can obtain $\mathbf{y}$ is unit vector and $\mathbf{y}\bot\mathbf{u}_1$. So $\mathbf{x,y}$ belong to subsets of the unit spheres in $\mathcal{R^n,R^m}$ respectively. As we assume above, $\lambda_1=\max z(\mathbf{x,y})$, therefore $\lambda_2\le\lambda_1$.</p>
  </li>
  <li>
    <p>Finally, we must prove $\lambda_i$ are nonnegative. We see that $z(\mathbf{x,y})=-z(\mathbf{-x,y})$. Obviously, $\max z(\mathbf{x,y})\ge0$.</p>
  </li>
</ul>

<h5 id="reference"><strong>Reference:</strong></h5>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>2.7</td>
          <td>Deep Learning</td>
          <td>Ian Goodfellow, Yoshua Bengio, Aaron Courville.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><a href="https://machinelearningcoban.com/2017/06/07/svd/" style="color:green">Bài 26: Singular Value Decomposition</a></li>
  <li><a href="http://db.cs.duke.edu/courses/cps111/spring07/notes/12.pdf" style="color:green">10. Singular Value Decomposition | Duke Computer Science</a></li>
</ul>

</div>
</div>

    </main>
    <footer class="page-footer teal">
      <div class="container">
        <div class="row">
          <div class="col s12">
            <img src="/assets/res/logo.png" alt="logo"/>
            <p class="grey-text text-lighten-4">Science Blog!
</p>
          </div>
        </div>
      </div>
      <div class="footer-copyright">
        <div class="container">
          &#xA9; 2020 Khoa Blog. All rights reserved. Powered by <a href="https://github.com/khoablog/khoablog.github.io">Khoa</a>.
        </div>
      </div>
    </footer>
    <script src="//code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/materialize/0.99.0/js/materialize.min.js"></script>
    
    
    
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})
      (window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-96359860-1', 'auto');
      ga('send', 'pageview');
    </script>
    
    <script src="/assets/js/main.js"></script>
  </body>
</html>