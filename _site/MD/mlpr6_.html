<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
      
      Khoa Blog
      
    </title>
    <link rel="shortcut icon" type="image/x-icon" href="/assets/res/favicon.ico">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/materialize/0.99.0/css/materialize.min.css">
    <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
    <link rel="stylesheet" href="/assets/css/main.css">
    
    
    <link rel="stylesheet" href="/assets/css/page.css">
    
    
    
    <link rel="stylesheet" href="/assets/css/syntax.css">
    <link rel="alternate" title="Beta" href="http://khoablogs.github.io">
    <link rel="sitemap" type="application/xml" title="Sitemap" href="/sitemap.xml">
    <!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Khoa Blog" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Science Blog!" />
<meta property="og:description" content="Science Blog!" />
<link rel="canonical" href="http://localhost:4000/MD/mlpr6_" />
<meta property="og:url" content="http://localhost:4000/MD/mlpr6_" />
<meta property="og:site_name" content="Khoa Blog" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Khoa Blog" />
<meta name="google-site-verification" content="UA-96359860-1" />
<script type="application/ld+json">
{"@type":"WebPage","url":"http://localhost:4000/MD/mlpr6_","headline":"Khoa Blog","description":"Science Blog!","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body>
    <header>
      <nav class="top-nav teal">
        <div class="nav-wrapper">
          <div class="container">
            <a class="page-title" href="/">Khoa Blog</a>
          </div>
        </div>
      </nav>
      <div class="container">
        <a href="#" data-activates="slide-out" class="button-collapse top-nav full hide-on-large-only">
          <i class="material-icons">menu</i>
        </a>
      </div>
      <ul id="slide-out" class="side-nav fixed">
        <li>
          <div class="userView">
            <div class="background"></div>
            <a href="https://github.com/khoatranrb" target="_blank"><img class="circle z-depth-2" src="/assets/res/user.png"></a>
            <span class="white-text name">Khoa Tran</span>
            <span class="white-text email">khoatranrb@gmail.com</span>
          </div>
        </li>
        <li><a class="waves-effect" href="/"><i class="material-icons">home</i>Home</a></li>
        <li><a class="waves-effect" href="/projects"><i class="material-icons">description</i>Projects</a></li>
        <li><a class="waves-effect" href="/categories"><i class="material-icons">sort</i>Categories</a></li>
        <li><a class="waves-effect" href="/tags"><i class="material-icons">label</i>Tags</a></li>
        <li><a class="waves-effect" href="http://khoablogs.github.io" target="_blank"><i class="material-icons">rss_feed</i>Beta</a></li>
        <li><div class="divider"></div></li>
        <li><a class="waves-effect" href="/about"><i class="material-icons">person</i>About</a></li>
        <li><a class="waves-effect" href="/contact"><i class="material-icons">email</i>Contact</a></li>
      </ul>
    </header>
    <main>

<div class="container">
  <div id="page-info">
  <h3></h3>
</div>
<div class="row">
  <ul>
  <li>In this blog, we will discuss about <strong>Information theory</strong>, about its concept and its mathematical nature.</li>
</ul>

<p>
    <p>
    <img src="https://raw.githubusercontent.com/khoatranrb/Img4Md/master/B/MLPR8/it.png" />
    <center><b>Fig1</b> <i>(Source: Wikipedia)</i></center>
</p>
</p>
<p><strong>Table of content:</strong></p>

<ul>
  <li><a href="#1">Entropy</a></li>
  <li><a href="#2">Joint entropy</a></li>
  <li><a href="#3">Condition entropy</a></li>
  <li><a href="#4">Mutial information</a></li>
  <li><a href="#5">Reference</a></li>
</ul>

<p><a name="1"></a></p>

<h5 id="1-entropy"><strong>1. Entropy</strong></h5>

<ul>
  <li>
    <p>Let consider a discrete random variable $x$. We wonder how much information is received when we observe a specific value for this variable. The amount of information can be viewed as the <strong>degree of surprise</strong> of $x$ or the <strong>uncertainty</strong> of variables.</p>
  </li>
  <li>
    <p>The intuition for entropy is that it is the average number of bits  required to represent or transmit an event drawn from the probability  distribution for the random variable.</p>
  </li>
  <li>
    <p>We use a quantity $h(x)$ as a measure for information content and the $h(x)$ must be a monotonic function of the probability distribution $p(x)$.</p>
  </li>
  <li>
    <p>If we have two events $x$ and $y$ that are unrelated, so $h(x,y)=h(x)+h(y)$. Cause $x$ and $y$ are independent, then $p(x,y)=p(x)p(y)$. From two relationships, we can show that:
\(h(x)=-\log p(x)\tag1\)
where the negative sign ensures that the value of $h(x)$ is positive or zero.</p>
  </li>
  <li>
    <p>The average amount of information of a random variable $X$ with distribution $p$:
\(\H[X]=-\sum_{x\in X}p(x)\log p(x)\tag2\)
called the <strong>entropy</strong> of the random variable $X$.</p>
  </li>
  <li>
    <p><strong>(A)</strong> We can see that, $\H[X]$ achieves the maximum when all of $p(x)$ are equal.</p>
  </li>
  <li>
    <p>In general, we have entropy of continuous variable:
\(\H[X]=-\int p(x)\log p(x)\text{d}x\tag3.\)</p>
  </li>
</ul>

<p><a name="2"></a></p>

<h5 id="2-joint-entropy"><strong>2. Joint Entropy</strong></h5>

<ul>
  <li><strong>Joint entropy</strong> is a measure of the uncertainty associated with a set of variables. It is the entropy of a joint probability distribution. With two distributions, it is defined by:</li>
</ul>

\[\H[(X,Y)]=-\int\int p(x,y)\log p(x,y)\text{d}x\text{d}y\tag4.\]

<p><a name="3"></a></p>

<h5 id="3-condition-entropy"><strong>3. Condition entropy</strong></h5>

<ul>
  <li>Consider a joint distribution $p(x,y)$. If a value of $x$ is already known, the average additional information needed to specify $y$ can be written as:
\(\begin{align}
\H[Y|X]&amp;=-\int p(y|X)\log p(y|X)\text{d}y\tag5\\
&amp;=-\int\int p(y,x)\log p(y|x)\text{d}y\text{d}x\tag6.
\end{align}\)
In other hand, we can write:
\(\begin{align}
\H[\mathbf{Y|X}]&amp;=-\int\int p(y,x)\log \frac{ p(y,x)}{p(x)}\text{d}y\text{d}x\tag6\\
&amp;=-\int\int p(y,x)\log p(y,x)\text{d}y\text{d}x+\int\int p(y,x)\log p(x)\text{d}y\text{d}x\tag7\\
&amp;=-\int\int p(y,x)\log p(y,x)\text{d}y\text{d}x+\int p(x)\log p(x)\text{d}x\tag8\\
&amp;=\H[(X,Y)]-\H[X]\tag{9}.
\end{align}\)
Thus, we have:
\(\H[(X,Y)]=\H[Y|X]+\H[X]=\H[X|Y]+\H[Y]\tag{10}.\)</li>
</ul>

<p><a name="4"></a></p>

<h5 id="4-mutual-information"><strong>4. Mutual information</strong></h5>

<ul>
  <li>
    <p>Mutual  information  is  a  quantity  that  measures  a  relationship  between  two random variables that are sampled simultaneously. It is  defined as:
\(\mathbb{I}[X;Y]=\int\int p(x,y)\log\frac{p(x,y)}{p(x)p(y)}\text{d}y\text{d}x\tag{11}\)</p>
  </li>
  <li>
    <p>We also have:
\(\begin{align}
\mathbb{I}[X;Y]&amp;=\int\int p(x,y)\log p(x,y)\text{d}x\text{d}y-\int\int p(x,y)\log p(x)\text{d}x\text{d}y-\int\int p(x,y)\log p(y)\text{d}x\text{d}y\tag{12}\\
&amp;=\H[X]+\H[Y]-\H[(X,Y)]\tag{13}\\
&amp;=\H[X]-\H[X|Y]\tag{14}\\
&amp;=\H[Y]-\H[Y|X]\tag{15}\\
&amp;=\H[(X,Y)]-\H[X|Y]-\H[Y|X]\tag{16}.
\end{align}\)</p>
  </li>
  <li>
    <p><strong>Mutual information</strong> is a symmetry function, so it can be used as a metric, that measures the same of two distribution.</p>
  </li>
</ul>

<p><a name="5"></a></p>

<h5 id="5-properties"><strong>5. Properties</strong></h5>

<ul>
  <li>
    <p>$\H[X]\ge0$. It is equal when $p(x)=1$ where $x\in X$. $(17)$</p>

    <p>Similarity, <strong>joint entropy</strong>, <strong>condition entropy</strong> and <strong>mutual information</strong> are also positive or zero.</p>
  </li>
  <li>
    <p>From $(10)$, we have $\H[(X,Y)]\ge\max{\H[X],\H[Y]}$. $(18)$</p>
  </li>
  <li>
    <p>From $(13)$, we have $\H[(X,Y)]\le\H[X]+\H[Y]$. $(19)$</p>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>From $(14)$ and $(15)$, we have $\H[X</td>
          <td>Y]\le\H[X]$ and $\H[Y</td>
          <td>X]\le\H[Y]$. $(20)$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>From $(12)$ and $(18)$, we have $\mathbb{I}[X;Y]\le\min{\H[X],\H[Y]}$. $(21)$</li>
</ul>

<h5 id="reference"><strong>Reference:</strong></h5>

<ul>
  <li>
    <p>[1] <a href="https://en.wikipedia.org/wiki/Entropy" style="color:green">Entropy | Wikipedia.</a></p>
  </li>
  <li>
    <p>[2] <a href="https://en.wikipedia.org/wiki/Joint_entropy" style="color:green">Joint Entropy | Wikipedia.</a></p>
  </li>
  <li>
    <p>[3] <a href="https://en.wikipedia.org/wiki/Conditional_entropy" style="color:green">Condition Entropy | Wikipedia.</a></p>
  </li>
  <li>
    <p>[4] <a href="https://en.wikipedia.org/wiki/Mutual_information" style="color:green">Mutual information | Wikipedia.</a></p>
  </li>
  <li>
    <p>[5] <a href=" https://people.cs.umass.edu/~elm/Teaching/Docs/mutInf.pdf" style="color:green">Entropy and Mutual Information | University of Massachusetts Amherst.</a></p>
  </li>
  <li>
    <p>[6] <a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/" style="color:green">Cross-entropy for machine learning | Machine Learning Mastery</a>.</p>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[7] 1.6</td>
          <td>Pattern Recognition and Machine Learning</td>
          <td>C.M. Bishop.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>[8] 2.8.1</td>
          <td>Machine Learning A Probabilistic Perspective</td>
          <td>K.P. Murphy</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>


</div>
</div>

    </main>
    <footer class="page-footer teal">
      <div class="container">
        <div class="row">
          <div class="col s12">
            <img src="/assets/res/logo.png" alt="logo"/>
            <p class="grey-text text-lighten-4">Science Blog!
</p>
          </div>
        </div>
      </div>
      <div class="footer-copyright">
        <div class="container">
          &#xA9; 2021 Khoa Blog. All rights reserved. Powered by <a href="https://github.com/khoablog/khoablog.github.io">Khoa</a>.
        </div>
      </div>
    </footer>
    <script src="//code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/materialize/0.99.0/js/materialize.min.js"></script>
    
    
    
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})
      (window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-96359860-1', 'auto');
      ga('send', 'pageview');
    </script>
    
    <script src="/assets/js/main.js"></script>
  </body>
</html>